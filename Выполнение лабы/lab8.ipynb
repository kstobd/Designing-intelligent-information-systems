{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import LambdaCallback\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все символы в нижний регистр\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../Датасеты/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим символы в char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " \"'\": 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " '*': 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " '0': 10,\n",
       " '3': 11,\n",
       " ':': 12,\n",
       " ';': 13,\n",
       " '?': 14,\n",
       " '[': 15,\n",
       " ']': 16,\n",
       " '_': 17,\n",
       " 'a': 18,\n",
       " 'b': 19,\n",
       " 'c': 20,\n",
       " 'd': 21,\n",
       " 'e': 22,\n",
       " 'f': 23,\n",
       " 'g': 24,\n",
       " 'h': 25,\n",
       " 'i': 26,\n",
       " 'j': 27,\n",
       " 'k': 28,\n",
       " 'l': 29,\n",
       " 'm': 30,\n",
       " 'n': 31,\n",
       " 'o': 32,\n",
       " 'p': 33,\n",
       " 'q': 34,\n",
       " 'r': 35,\n",
       " 's': 36,\n",
       " 't': 37,\n",
       " 'u': 38,\n",
       " 'v': 39,\n",
       " 'w': 40,\n",
       " 'x': 41,\n",
       " 'y': 42,\n",
       " 'z': 43,\n",
       " 'ù': 44,\n",
       " '—': 45,\n",
       " '‘': 46,\n",
       " '’': 47,\n",
       " '“': 48,\n",
       " '”': 49,\n",
       " '\\ufeff': 50}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуем данные\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144678\n",
      "Total Vocab:  51\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготавливаем тренировочные данные разделяя всю книгу на последовательности из 100 символов и в качестве ответа выбирая символ следующий за выбранными\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144578\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "нормализуем данные\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144578, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем архитектуру сети\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 20:46:24.834457: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-12-19 20:46:24.835208: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-12-19 20:46:24.835868: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как у нас нет тестовыхх данных, то создаем чекпоинты что бы как то ориентироваться в обучении модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем инт в чар\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем свой CallBack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(epoch, logs):\n",
    "    if (epoch + 1) % 4 == 0:\n",
    "        print(f\"\\n--- Generating text after epoch {epoch + 1} ---\")\n",
    "\n",
    "        start_index = np.random.randint(0, len(dataX) - 1)\n",
    "        pattern = dataX[start_index]\n",
    "        print(\"Seed:\")\n",
    "        print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "        generated_text = \"\"\n",
    "        for i in range(200):\n",
    "            x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "            x = x / float(n_vocab)\n",
    "            prediction = model.predict(x, verbose=0)\n",
    "            index = np.argmax(prediction)\n",
    "            result = int_to_char[index]\n",
    "            generated_text += result\n",
    "\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "\n",
    "        print(\"Generated text:\")\n",
    "        print(generated_text)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "text_generation_callback = LambdaCallback(on_epoch_end=generate_text)\n",
    "\n",
    "callbacks_list.append(text_generation_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      3\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs8lab/fit/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(\n\u001b[1;32m      5\u001b[0m     log_dir\u001b[38;5;241m=\u001b[39mlog_dir, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m callbacks_list\u001b[38;5;241m.\u001b[39mappend(tensorboard_callback)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "log_dir = (\".\" +\n",
    "           \"logs8lab/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list.append(tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.ModelCheckpoint at 0x7fb88b4adeb0>,\n",
       " <keras.callbacks.LambdaCallback at 0x7fb88bccc2b0>,\n",
       " <keras.callbacks.TensorBoard at 0x7fb88a1299d0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 20:46:25.214557: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-12-19 20:46:25.215809: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-12-19 20:46:25.216636: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-12-19 20:46:25.571051: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-12-19 20:46:25.571831: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-12-19 20:46:25.572596: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-12-19 20:46:25.835246: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-19 20:46:25.931405: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_10.\n",
      "2024-12-19 20:46:26.047142: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-19 20:46:26.257747: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130/1130 [==============================] - ETA: 0s - loss: 3.0173\n",
      "Epoch 1: loss improved from inf to 3.01729, saving model to weights-improvement-01-3.0173.hdf5\n",
      "1130/1130 [==============================] - 56s 48ms/step - loss: 3.0173\n",
      "Epoch 2/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.8258\n",
      "Epoch 2: loss improved from 3.01729 to 2.82578, saving model to weights-improvement-02-2.8258.hdf5\n",
      "1130/1130 [==============================] - 53s 47ms/step - loss: 2.8258\n",
      "Epoch 3/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.7273\n",
      "Epoch 3: loss improved from 2.82578 to 2.72726, saving model to weights-improvement-03-2.7273.hdf5\n",
      "1130/1130 [==============================] - 52s 46ms/step - loss: 2.7273\n",
      "Epoch 4/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.6504\n",
      "Epoch 4: loss improved from 2.72726 to 2.65036, saving model to weights-improvement-04-2.6504.hdf5\n",
      "\n",
      "--- Generating text after epoch 4 ---\n",
      "Seed:\n",
      "\" s it went,\n",
      "“one side will make you grow taller, and the other side will make you\n",
      "grow shorter.”\n",
      "\n",
      "“on \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 20:49:58.038434: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-12-19 20:49:58.039153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-12-19 20:49:58.040127: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-12-19 20:49:58.069337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-19 20:49:58.149670: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " wou do woe toue to tee ”ou ”he cat ”ou ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu ”hu \n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 62s 55ms/step - loss: 2.6504\n",
      "Epoch 5/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.5825\n",
      "Epoch 5: loss improved from 2.65036 to 2.58247, saving model to weights-improvement-05-2.5825.hdf5\n",
      "1130/1130 [==============================] - 54s 48ms/step - loss: 2.5825\n",
      "Epoch 6/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.5207\n",
      "Epoch 6: loss improved from 2.58247 to 2.52075, saving model to weights-improvement-06-2.5207.hdf5\n",
      "1130/1130 [==============================] - 53s 47ms/step - loss: 2.5207\n",
      "Epoch 7/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.4669\n",
      "Epoch 7: loss improved from 2.52075 to 2.46692, saving model to weights-improvement-07-2.4669.hdf5\n",
      "1130/1130 [==============================] - 55s 48ms/step - loss: 2.4669\n",
      "Epoch 8/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.4189\n",
      "Epoch 8: loss improved from 2.46692 to 2.41889, saving model to weights-improvement-08-2.4189.hdf5\n",
      "\n",
      "--- Generating text after epoch 8 ---\n",
      "Seed:\n",
      "\" ed\n",
      "around it—once more the shriek of the gryphon, the squeaking of the\n",
      "lizard’s slate-pencil, and th \"\n",
      "Generated text:\n",
      "e toiee to tee toeee an in would the tas io toene an the cad no the cad no the cad no the cad no the cad no the cad no the cad no the cad no the cad no the cad no the cad no the cad no the cad no the \n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 70s 62ms/step - loss: 2.4189\n",
      "Epoch 9/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.3755\n",
      "Epoch 9: loss improved from 2.41889 to 2.37546, saving model to weights-improvement-09-2.3755.hdf5\n",
      "1130/1130 [==============================] - 53s 47ms/step - loss: 2.3755\n",
      "Epoch 10/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.3341\n",
      "Epoch 10: loss improved from 2.37546 to 2.33412, saving model to weights-improvement-10-2.3341.hdf5\n",
      "1130/1130 [==============================] - 54s 48ms/step - loss: 2.3341\n",
      "Epoch 11/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.2960\n",
      "Epoch 11: loss improved from 2.33412 to 2.29597, saving model to weights-improvement-11-2.2960.hdf5\n",
      "1130/1130 [==============================] - 59s 52ms/step - loss: 2.2960\n",
      "Epoch 12/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.2599\n",
      "Epoch 12: loss improved from 2.29597 to 2.25985, saving model to weights-improvement-12-2.2599.hdf5\n",
      "\n",
      "--- Generating text after epoch 12 ---\n",
      "Seed:\n",
      "\" ere or might have been was not\n",
      "otherwise than what you had been would have appeared to them to be\n",
      "ot \"\n",
      "Generated text:\n",
      " the thet  she had toen i thsl the horse  a dat ro the tooe  f dad tottd the tooed of the morer of the tonee  and the tas anl the horre oo the tas ho the tooee  she was toen i thil the hodr the had be\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 75s 67ms/step - loss: 2.2599\n",
      "Epoch 13/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.2221\n",
      "Epoch 13: loss improved from 2.25985 to 2.22207, saving model to weights-improvement-13-2.2221.hdf5\n",
      "1130/1130 [==============================] - 69s 61ms/step - loss: 2.2221\n",
      "Epoch 14/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.1874\n",
      "Epoch 14: loss improved from 2.22207 to 2.18744, saving model to weights-improvement-14-2.1874.hdf5\n",
      "1130/1130 [==============================] - 61s 54ms/step - loss: 2.1874\n",
      "Epoch 15/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.1546\n",
      "Epoch 15: loss improved from 2.18744 to 2.15461, saving model to weights-improvement-15-2.1546.hdf5\n",
      "1130/1130 [==============================] - 67s 60ms/step - loss: 2.1546\n",
      "Epoch 16/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.1232\n",
      "Epoch 16: loss improved from 2.15461 to 2.12322, saving model to weights-improvement-16-2.1232.hdf5\n",
      "\n",
      "--- Generating text after epoch 16 ---\n",
      "Seed:\n",
      "\" d back to the little\n",
      "door: but, alas! the little door was shut again, and the little golden\n",
      "key was  \"\n",
      "Generated text:\n",
      "so the toier  she was not io the wine see was so the tooe  and taid to aeicr  she cad nott the dare and the sabbit was she was so tee the was so tee the was so tee the was so tee the was so tee the wa\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 74s 66ms/step - loss: 2.1232\n",
      "Epoch 17/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.0919\n",
      "Epoch 17: loss improved from 2.12322 to 2.09187, saving model to weights-improvement-17-2.0919.hdf5\n",
      "1130/1130 [==============================] - 62s 55ms/step - loss: 2.0919\n",
      "Epoch 18/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.0633\n",
      "Epoch 18: loss improved from 2.09187 to 2.06332, saving model to weights-improvement-18-2.0633.hdf5\n",
      "1130/1130 [==============================] - 59s 53ms/step - loss: 2.0633\n",
      "Epoch 19/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.0401\n",
      "Epoch 19: loss improved from 2.06332 to 2.04009, saving model to weights-improvement-19-2.0401.hdf5\n",
      "1130/1130 [==============================] - 58s 51ms/step - loss: 2.0401\n",
      "Epoch 20/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.0186\n",
      "Epoch 20: loss improved from 2.04009 to 2.01863, saving model to weights-improvement-20-2.0186.hdf5\n",
      "\n",
      "--- Generating text after epoch 20 ---\n",
      "Seed:\n",
      "\" ords:—\n",
      "\n",
      "“i speak severely to my boy,\n",
      "    i beat him when he sneezes;\n",
      "for he can thoroughly enjoy\n",
      "    \"\n",
      "Generated text:\n",
      "  beautioui the garce. \n",
      "\n",
      "  chapter ii.the warl wfil she was no the gande hare and the winle gar and the winle gar \n",
      "she cad nott the dinrt to tee thet she had been whth the winle har and thine whe hoek\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 70s 62ms/step - loss: 2.0186\n",
      "Epoch 21/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.9843\n",
      "Epoch 21: loss improved from 2.01863 to 1.98426, saving model to weights-improvement-21-1.9843.hdf5\n",
      "1130/1130 [==============================] - 56s 50ms/step - loss: 1.9843\n",
      "Epoch 22/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.9592\n",
      "Epoch 22: loss improved from 1.98426 to 1.95919, saving model to weights-improvement-22-1.9592.hdf5\n",
      "1130/1130 [==============================] - 57s 51ms/step - loss: 1.9592\n",
      "Epoch 23/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.9433\n",
      "Epoch 23: loss improved from 1.95919 to 1.94325, saving model to weights-improvement-23-1.9433.hdf5\n",
      "1130/1130 [==============================] - 58s 51ms/step - loss: 1.9433\n",
      "Epoch 24/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.0422\n",
      "Epoch 24: loss did not improve from 1.94325\n",
      "\n",
      "--- Generating text after epoch 24 ---\n",
      "Seed:\n",
      "\" .\n",
      "\n",
      "the caterpillar was the first to speak.\n",
      "\n",
      "“what size do you want to be?” it asked.\n",
      "\n",
      "“oh, i’m not p \"\n",
      "Generated text:\n",
      "oae to aatert i cater,” shi gatter senli dorsertly, “oo toued io the morshen ”\n",
      "\n",
      "“h con’t know the weyt ca ateat?” said the caterpillar.\n",
      "\n",
      "“ie hourte toen io the sore!” said the mock turtle, “in and the\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 68s 60ms/step - loss: 2.0422\n",
      "Epoch 25/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.9223\n",
      "Epoch 25: loss improved from 1.94325 to 1.92226, saving model to weights-improvement-25-1.9223.hdf5\n",
      "1130/1130 [==============================] - 53s 47ms/step - loss: 1.9223\n",
      "Epoch 26/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8833\n",
      "Epoch 26: loss improved from 1.92226 to 1.88326, saving model to weights-improvement-26-1.8833.hdf5\n",
      "1130/1130 [==============================] - 54s 48ms/step - loss: 1.8833\n",
      "Epoch 27/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8643\n",
      "Epoch 27: loss improved from 1.88326 to 1.86432, saving model to weights-improvement-27-1.8643.hdf5\n",
      "1130/1130 [==============================] - 55s 48ms/step - loss: 1.8643\n",
      "Epoch 28/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8588\n",
      "Epoch 28: loss improved from 1.86432 to 1.85876, saving model to weights-improvement-28-1.8588.hdf5\n",
      "\n",
      "--- Generating text after epoch 28 ---\n",
      "Seed:\n",
      "\"  it?” said\n",
      "the march hare.\n",
      "\n",
      "“exactly so,” said alice.\n",
      "\n",
      "“then you should say what you mean,” the marc \"\n",
      "Generated text:\n",
      "h hare said to alice, tooningny and toine tiieg was io a mirtle tfre \n",
      "she was thi gar hn the wind \n",
      "she was toiteigg oo the thidg was shet iid to the thre and tas ior hoad \n",
      "the had neter leke to be a l\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 65s 58ms/step - loss: 1.8588\n",
      "Epoch 29/80\n",
      "1129/1130 [============================>.] - ETA: 0s - loss: 1.8610\n",
      "Epoch 29: loss did not improve from 1.85876\n",
      "1130/1130 [==============================] - 54s 47ms/step - loss: 1.8611\n",
      "Epoch 30/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8511\n",
      "Epoch 30: loss improved from 1.85876 to 1.85112, saving model to weights-improvement-30-1.8511.hdf5\n",
      "1130/1130 [==============================] - 51s 45ms/step - loss: 1.8511\n",
      "Epoch 31/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8516\n",
      "Epoch 31: loss did not improve from 1.85112\n",
      "1130/1130 [==============================] - 51s 45ms/step - loss: 1.8516\n",
      "Epoch 32/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 2.1062\n",
      "Epoch 32: loss did not improve from 1.85112\n",
      "\n",
      "--- Generating text after epoch 32 ---\n",
      "Seed:\n",
      "\" , and on both sides of it, and\n",
      "behind it, it occurred to her that she might as well look and see wha \"\n",
      "Generated text:\n",
      "t the had bou doon thth the was so tas the had base and the was so tas then the was to sas then the was to sas then i sas do a ging anne tat an in woue, “in would neve tointen to tea the had banse the\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 60s 53ms/step - loss: 2.1062\n",
      "Epoch 33/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8708\n",
      "Epoch 33: loss did not improve from 1.85112\n",
      "1130/1130 [==============================] - 52s 46ms/step - loss: 1.8708\n",
      "Epoch 34/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8292\n",
      "Epoch 34: loss improved from 1.85112 to 1.82920, saving model to weights-improvement-34-1.8292.hdf5\n",
      "1130/1130 [==============================] - 51s 46ms/step - loss: 1.8292\n",
      "Epoch 35/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8235\n",
      "Epoch 35: loss improved from 1.82920 to 1.82346, saving model to weights-improvement-35-1.8235.hdf5\n",
      "1130/1130 [==============================] - 52s 46ms/step - loss: 1.8235\n",
      "Epoch 36/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.8141\n",
      "Epoch 36: loss improved from 1.82346 to 1.81407, saving model to weights-improvement-36-1.8141.hdf5\n",
      "\n",
      "--- Generating text after epoch 36 ---\n",
      "Seed:\n",
      "\" d the rabbit angrily. “here! come and\n",
      "help me out of _this!_” (sounds of more broken glass.)\n",
      "\n",
      "“now t \"\n",
      "Generated text:\n",
      "as at the wirte teree to saie,” said alice, “ard io they have the mors oo the toon oo the saad to the horse, and the dare pald in a loedt hingt oo then iid tas iore to be in whth the cour, and the the\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 62s 55ms/step - loss: 1.8141\n",
      "Epoch 37/80\n",
      "1129/1130 [============================>.] - ETA: 0s - loss: 1.8059\n",
      "Epoch 37: loss improved from 1.81407 to 1.80611, saving model to weights-improvement-37-1.8061.hdf5\n",
      "1130/1130 [==============================] - 52s 46ms/step - loss: 1.8061\n",
      "Epoch 38/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.7970\n",
      "Epoch 38: loss improved from 1.80611 to 1.79704, saving model to weights-improvement-38-1.7970.hdf5\n",
      "1130/1130 [==============================] - 53s 47ms/step - loss: 1.7970\n",
      "Epoch 39/80\n",
      "1129/1130 [============================>.] - ETA: 0s - loss: 1.7806\n",
      "Epoch 39: loss improved from 1.79704 to 1.78053, saving model to weights-improvement-39-1.7805.hdf5\n",
      "1130/1130 [==============================] - 54s 48ms/step - loss: 1.7805\n",
      "Epoch 40/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.7711\n",
      "Epoch 40: loss improved from 1.78053 to 1.77106, saving model to weights-improvement-40-1.7711.hdf5\n",
      "\n",
      "--- Generating text after epoch 40 ---\n",
      "Seed:\n",
      "\"  again?” alice ventured\n",
      "to ask.\n",
      "\n",
      "“suppose we change the subject,” the march hare interrupted, yawnin \"\n",
      "Generated text:\n",
      "g \n",
      "thr ano lend oo eonner on the courdr. and the seiei en the thieg hor ho the goakn, and whnt saed to tee th the gorpeon in the court, wie oueked the sooe of the saaeit was on the tood. \n",
      "“h can tour \n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 64s 57ms/step - loss: 1.7711\n",
      "Epoch 41/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.7562\n",
      "Epoch 41: loss improved from 1.77106 to 1.75624, saving model to weights-improvement-41-1.7562.hdf5\n",
      "1130/1130 [==============================] - 52s 46ms/step - loss: 1.7562\n",
      "Epoch 42/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.7458\n",
      "Epoch 42: loss improved from 1.75624 to 1.74581, saving model to weights-improvement-42-1.7458.hdf5\n",
      "1130/1130 [==============================] - 54s 47ms/step - loss: 1.7458\n",
      "Epoch 43/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.9130\n",
      "Epoch 43: loss did not improve from 1.74581\n",
      "1130/1130 [==============================] - 55s 49ms/step - loss: 1.9130\n",
      "Epoch 44/80\n",
      "1129/1130 [============================>.] - ETA: 0s - loss: 1.7603\n",
      "Epoch 44: loss did not improve from 1.74581\n",
      "\n",
      "--- Generating text after epoch 44 ---\n",
      "Seed:\n",
      "\" ng down at her with large round eyes, and\n",
      "feebly stretching out one paw, trying to touch her. “poor  \"\n",
      "Generated text:\n",
      "sotnd oo tou,” \n",
      "“i con’t knke the woide of the soede tfin the seit,” said the macth  andce weiy onmily at the could \n",
      "toe the was salking the rigt and thene was to toekk \n",
      "“ho thsh the sey,” the macc to\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 64s 56ms/step - loss: 1.7604\n",
      "Epoch 45/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.7098\n",
      "Epoch 45: loss improved from 1.74581 to 1.70982, saving model to weights-improvement-45-1.7098.hdf5\n",
      "1130/1130 [==============================] - 55s 48ms/step - loss: 1.7098\n",
      "Epoch 46/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6930\n",
      "Epoch 46: loss improved from 1.70982 to 1.69304, saving model to weights-improvement-46-1.6930.hdf5\n",
      "1130/1130 [==============================] - 56s 49ms/step - loss: 1.6930\n",
      "Epoch 47/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6836\n",
      "Epoch 47: loss improved from 1.69304 to 1.68355, saving model to weights-improvement-47-1.6836.hdf5\n",
      "1130/1130 [==============================] - 56s 49ms/step - loss: 1.6836\n",
      "Epoch 48/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6861\n",
      "Epoch 48: loss did not improve from 1.68355\n",
      "\n",
      "--- Generating text after epoch 48 ---\n",
      "Seed:\n",
      "\"  children who had got burnt, and\n",
      "eaten up by wild beasts and other unpleasant things, all because th \"\n",
      "Generated text:\n",
      "e was soingie the sioe \n",
      "the had neter dern the soin afdon, and said an it was nerenilyg aeainst the hoore of the goush, and the tored a lottle oo the shate\n",
      "was soenking to sei it hnr hort th the oooe \n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 66s 59ms/step - loss: 1.6861\n",
      "Epoch 49/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6766\n",
      "Epoch 49: loss improved from 1.68355 to 1.67662, saving model to weights-improvement-49-1.6766.hdf5\n",
      "1130/1130 [==============================] - 55s 48ms/step - loss: 1.6766\n",
      "Epoch 50/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6810\n",
      "Epoch 50: loss did not improve from 1.67662\n",
      "1130/1130 [==============================] - 55s 49ms/step - loss: 1.6810\n",
      "Epoch 51/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6805\n",
      "Epoch 51: loss did not improve from 1.67662\n",
      "1130/1130 [==============================] - 56s 49ms/step - loss: 1.6805\n",
      "Epoch 52/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6745\n",
      "Epoch 52: loss improved from 1.67662 to 1.67453, saving model to weights-improvement-52-1.6745.hdf5\n",
      "\n",
      "--- Generating text after epoch 52 ---\n",
      "Seed:\n",
      "\" all?” said the footman. “that’s the first\n",
      "question, you know.”\n",
      "\n",
      "it was, no doubt: only alice did not \"\n",
      "Generated text:\n",
      " like to ge wou thrh at the had oate and the wan oote tn bn the carerp the call ar wall an if she tas oo tie tire and the whnle taede on teat woued her head and thet sas doon the winde har and the cad\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 66s 59ms/step - loss: 1.6745\n",
      "Epoch 53/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.9206\n",
      "Epoch 53: loss did not improve from 1.67453\n",
      "1130/1130 [==============================] - 55s 49ms/step - loss: 1.9206\n",
      "Epoch 54/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.7277\n",
      "Epoch 54: loss did not improve from 1.67453\n",
      "1130/1130 [==============================] - 58s 51ms/step - loss: 1.7277\n",
      "Epoch 55/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6603\n",
      "Epoch 55: loss improved from 1.67453 to 1.66027, saving model to weights-improvement-55-1.6603.hdf5\n",
      "1130/1130 [==============================] - 58s 52ms/step - loss: 1.6603\n",
      "Epoch 56/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6559\n",
      "Epoch 56: loss improved from 1.66027 to 1.65587, saving model to weights-improvement-56-1.6559.hdf5\n",
      "\n",
      "--- Generating text after epoch 56 ---\n",
      "Seed:\n",
      "\" , frowning, but very politely: “did\n",
      "you speak?”\n",
      "\n",
      "“not i!” said the lory hastily.\n",
      "\n",
      "“i thought you did \"\n",
      "Generated text:\n",
      " ”our marehn,” said the macch  and to aeden io an a goer  soeke a ait df drowued an in go a goeat herl that she had neter leat the rooe of the sare to see that she had sote her eeee, and she though he\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 68s 61ms/step - loss: 1.6559\n",
      "Epoch 57/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6463\n",
      "Epoch 57: loss improved from 1.65587 to 1.64631, saving model to weights-improvement-57-1.6463.hdf5\n",
      "1130/1130 [==============================] - 56s 50ms/step - loss: 1.6463\n",
      "Epoch 58/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6496\n",
      "Epoch 58: loss did not improve from 1.64631\n",
      "1130/1130 [==============================] - 59s 52ms/step - loss: 1.6496\n",
      "Epoch 59/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6440\n",
      "Epoch 59: loss improved from 1.64631 to 1.64396, saving model to weights-improvement-59-1.6440.hdf5\n",
      "1130/1130 [==============================] - 60s 53ms/step - loss: 1.6440\n",
      "Epoch 60/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6324\n",
      "Epoch 60: loss improved from 1.64396 to 1.63239, saving model to weights-improvement-60-1.6324.hdf5\n",
      "\n",
      "--- Generating text after epoch 60 ---\n",
      "Seed:\n",
      "\" e had this fit_—’ you never had fits, my dear, i\n",
      "think?” he said to the queen.\n",
      "\n",
      "“never!” said the qu \"\n",
      "Generated text:\n",
      "een, and the jrrkedid aedin wint an if eoedgee to herd the soo of the soeee ofe was gore boeserstthits herself it aalen to hen that she was not inr horo the dard ooe to thet wored hars noekkid hor, an\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 68s 61ms/step - loss: 1.6324\n",
      "Epoch 61/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6275\n",
      "Epoch 61: loss improved from 1.63239 to 1.62749, saving model to weights-improvement-61-1.6275.hdf5\n",
      "1130/1130 [==============================] - 57s 51ms/step - loss: 1.6275\n",
      "Epoch 62/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6203\n",
      "Epoch 62: loss improved from 1.62749 to 1.62027, saving model to weights-improvement-62-1.6203.hdf5\n",
      "1130/1130 [==============================] - 59s 52ms/step - loss: 1.6203\n",
      "Epoch 63/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6106\n",
      "Epoch 63: loss improved from 1.62027 to 1.61056, saving model to weights-improvement-63-1.6106.hdf5\n",
      "1130/1130 [==============================] - 81s 71ms/step - loss: 1.6106\n",
      "Epoch 64/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6031\n",
      "Epoch 64: loss improved from 1.61056 to 1.60314, saving model to weights-improvement-64-1.6031.hdf5\n",
      "\n",
      "--- Generating text after epoch 64 ---\n",
      "Seed:\n",
      "\" .\n",
      "the pool of tears\n",
      "\n",
      "\n",
      "“curiouser and curiouser!” cried alice (she was so much surprised, that\n",
      "for th \"\n",
      "Generated text:\n",
      "e was solegsinsg to be thet of the was sotn the dare one at tais as the could, and sas soing on the shade  the was not oo sig winl whe had soeeeed at the waited ofte and the courd, aud then she was so\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 113s 100ms/step - loss: 1.6031\n",
      "Epoch 65/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.6012\n",
      "Epoch 65: loss improved from 1.60314 to 1.60116, saving model to weights-improvement-65-1.6012.hdf5\n",
      "1130/1130 [==============================] - 105s 93ms/step - loss: 1.6012\n",
      "Epoch 66/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5872\n",
      "Epoch 66: loss improved from 1.60116 to 1.58716, saving model to weights-improvement-66-1.5872.hdf5\n",
      "1130/1130 [==============================] - 84s 74ms/step - loss: 1.5872\n",
      "Epoch 67/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5850\n",
      "Epoch 67: loss improved from 1.58716 to 1.58496, saving model to weights-improvement-67-1.5850.hdf5\n",
      "1130/1130 [==============================] - 90s 79ms/step - loss: 1.5850\n",
      "Epoch 68/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5759\n",
      "Epoch 68: loss improved from 1.58496 to 1.57588, saving model to weights-improvement-68-1.5759.hdf5\n",
      "\n",
      "--- Generating text after epoch 68 ---\n",
      "Seed:\n",
      "\" und\n",
      "in her life; it was all ridges and furrows; the balls were live\n",
      "hedgehogs, the mallets live flam \"\n",
      "Generated text:\n",
      "eed then she had boen was  “ho would belled to toit!”\n",
      "and teate thit samdn aafut fnon at her fand on hir feaed. and whe hnrped to the wonde hurd beai anl a lirgle lo the tarl was avinling \n",
      "thr aio tot\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 99s 88ms/step - loss: 1.5759\n",
      "Epoch 69/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5729\n",
      "Epoch 69: loss improved from 1.57588 to 1.57288, saving model to weights-improvement-69-1.5729.hdf5\n",
      "1130/1130 [==============================] - 97s 86ms/step - loss: 1.5729\n",
      "Epoch 70/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5643\n",
      "Epoch 70: loss improved from 1.57288 to 1.56434, saving model to weights-improvement-70-1.5643.hdf5\n",
      "1130/1130 [==============================] - 83s 73ms/step - loss: 1.5643\n",
      "Epoch 71/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5562\n",
      "Epoch 71: loss improved from 1.56434 to 1.55624, saving model to weights-improvement-71-1.5562.hdf5\n",
      "1130/1130 [==============================] - 88s 78ms/step - loss: 1.5562\n",
      "Epoch 72/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5517\n",
      "Epoch 72: loss improved from 1.55624 to 1.55171, saving model to weights-improvement-72-1.5517.hdf5\n",
      "\n",
      "--- Generating text after epoch 72 ---\n",
      "Seed:\n",
      "\" \n",
      "but on the whole she thought it would be quite as safe to stay with it\n",
      "as to go after that savage q \"\n",
      "Generated text:\n",
      "o thl oneer way on the thole harder, shet ser notking hor the thile was soinkie the titeo saed an her sith, and the thele sabbet wutle beain tfe ridete of the soml. \n",
      "“the brrmous yit aoe _veu?” said a\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 98s 87ms/step - loss: 1.5517\n",
      "Epoch 73/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5404\n",
      "Epoch 73: loss improved from 1.55171 to 1.54042, saving model to weights-improvement-73-1.5404.hdf5\n",
      "1130/1130 [==============================] - 80s 71ms/step - loss: 1.5404\n",
      "Epoch 74/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5381\n",
      "Epoch 74: loss improved from 1.54042 to 1.53812, saving model to weights-improvement-74-1.5381.hdf5\n",
      "1130/1130 [==============================] - 81s 71ms/step - loss: 1.5381\n",
      "Epoch 75/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5390\n",
      "Epoch 75: loss did not improve from 1.53812\n",
      "1130/1130 [==============================] - 90s 80ms/step - loss: 1.5390\n",
      "Epoch 76/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5285\n",
      "Epoch 76: loss improved from 1.53812 to 1.52847, saving model to weights-improvement-76-1.5285.hdf5\n",
      "\n",
      "--- Generating text after epoch 76 ---\n",
      "Seed:\n",
      "\"  on, “what _have_ you been doing\n",
      "here?”\n",
      "\n",
      "“may it please your majesty,” said two, in a very humble to \"\n",
      "Generated text:\n",
      "ne, foingng aling tine al clgcer oo the coore. \n",
      "“ie you’de thersn the mers oisele,” said alice. “i’ve gerd oo a goed ture wo than yo\n",
      "the wan note toaere! in doarntt wi tea theer toenent, bot i coo’t g\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 101s 90ms/step - loss: 1.5285\n",
      "Epoch 77/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5243\n",
      "Epoch 77: loss improved from 1.52847 to 1.52429, saving model to weights-improvement-77-1.5243.hdf5\n",
      "1130/1130 [==============================] - 93s 83ms/step - loss: 1.5243\n",
      "Epoch 78/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5210\n",
      "Epoch 78: loss improved from 1.52429 to 1.52099, saving model to weights-improvement-78-1.5210.hdf5\n",
      "1130/1130 [==============================] - 88s 78ms/step - loss: 1.5210\n",
      "Epoch 79/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5163\n",
      "Epoch 79: loss improved from 1.52099 to 1.51635, saving model to weights-improvement-79-1.5163.hdf5\n",
      "1130/1130 [==============================] - 78s 69ms/step - loss: 1.5163\n",
      "Epoch 80/80\n",
      "1130/1130 [==============================] - ETA: 0s - loss: 1.5065\n",
      "Epoch 80: loss improved from 1.51635 to 1.50649, saving model to weights-improvement-80-1.5065.hdf5\n",
      "\n",
      "--- Generating text after epoch 80 ---\n",
      "Seed:\n",
      "\"  the race was over. however,\n",
      "when they had been running half an hour or so, and were quite dry\n",
      "again \"\n",
      "Generated text:\n",
      ", the konked at the shoue to cid an wplde bn on hakken to her hnte, and whin she hod bnl the thing rabbit suine that sheel war shanrngg to soeak of the garden, she kouke he raed to her hard onte thmee\n",
      "\n",
      "\n",
      "1130/1130 [==============================] - 92s 82ms/step - loss: 1.5065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb88b497e20>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=80, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117541 data lenght 144578\n",
      "[22, 24, 26, 31, 31, 26, 31, 24, 1, 23, 35, 32, 30, 1, 37, 25, 26, 36, 1, 30, 32, 35, 31, 26, 31, 24, 7, 49, 1, 36, 18, 26, 21, 0, 18, 29, 26, 20, 22, 1, 18, 1, 29, 26, 37, 37, 29, 22, 1, 37, 26, 30, 26, 21, 29, 42, 12, 1, 48, 19, 38, 37, 1, 26, 37, 47, 36, 1, 31, 32, 1, 38, 36, 22, 1, 24, 32, 26, 31, 24, 1, 19, 18, 20, 28, 1, 37, 32, 1, 42, 22, 36, 37, 22, 35, 21, 18, 42, 7, 0]\n",
      "Seed:\n",
      "\" eginning from this morning,” said\n",
      "alice a little timidly: “but it’s no use going back to yesterday,\n",
      " \"\n",
      "bedruiott soey iir soued—.\n",
      "aod was io alouh simeg an toe fiotens and the carye herlel. and the courd not was ano rired and arlend tha winl she ged not thi was soinging to be toen as oees, and was gorn to her in so shat it was anl aeling the hingeh wfth the tar anwndusd,\n",
      "the pagb th they har and tha tiol the had shiter and thint shat sie was gows and something ald then she had toiened to thenk the was gown to inne th the oooerssn ana thene was toenened to siink to her onre. \n",
      "“h wasd i dad oo dien iitt as alg,” said alice, “whu ii you meee tu tuoke. and yhat is mes herd ”ou so ge rald.”\n",
      "\n",
      "“what _ie__!” said the muck turtle an an sfell ii andige th thes: “ih you doe yould be aneled in a woide? fus i tasd’t ger hen oo toe than a tiing oe mete ferse oh then!”\n",
      "\n",
      "“h most whtt mute rr ei,” said the mock turtle an in speee “ith she suher. “ho’s a gatce hare wath _y the hord, she madg on tu hend that she was gow a cong somes an once, \n",
      "“he ooss di fas kest io the seme whing a goes,” the march hare \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(dataX)-1)\n",
    "print(start, \"data lenght\", len(dataX))\n",
    "pattern = dataX[start]\n",
    "# got one array\n",
    "print(pattern)\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    # sys.stdout.write(result)\n",
    "    print(result, end=\"\")\n",
    "    pattern.append(index)\n",
    "    # print(pattern)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlog_dir\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_dir' is not defined"
     ]
    }
   ],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log_dir = '/Users/kst_obd/test_folder_for_tesorboard/logs8lab/fit/20241219-204624/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 33571), started 0:00:05 ago. (Use '!kill 33571' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9465bab3c3370459\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9465bab3c3370459\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {new_log_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
